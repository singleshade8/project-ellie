import torch
import sounddevice as sd
import numpy as np
import webrtcvad
import queue
import sys
import wave
import os
from transformers import pipeline
import joblib
import openai
from dotenv import load_dotenv

# Load environment variables
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# Constants
RATE = 16000
FRAME_DURATION = 30  # ms
FRAME_SIZE = int(RATE * FRAME_DURATION / 1000)
MIN_CHUNK_DURATION = 0.1  # seconds
VAD_MODE = 1  # 0=aggressive, 3=very strict

# VAD
vad = webrtcvad.Vad(VAD_MODE)

# ASR pipeline with Whisper (English only)
asr = pipeline(
    "automatic-speech-recognition",
    model="openai/whisper-base.en",
    device=0 if torch.cuda.is_available() else -1,
)

# Sentiment analysis
sentiment = pipeline(
    "sentiment-analysis",
    device=0 if torch.cuda.is_available() else -1
)

# Intent classification (optional)
try:
    label_encoder = joblib.load("label_encoder.pkl")
    from transformers import DistilBertForSequenceClassification, DistilBertTokenizer
    tokenizer = DistilBertTokenizer.from_pretrained("distilbert-base-uncased")
    intent_model = DistilBertForSequenceClassification.from_pretrained("intent_model").to(
        "cuda" if torch.cuda.is_available() else "cpu"
    )
    print("âœ… Intent model loaded")
except:
    label_encoder = None
    intent_model = None
    print("âš ï¸ Intent model missing, continuing without")

q = queue.Queue()

# GPT suggestion helper
def get_suggestion_from_gpt(intent, sentiment, text):
    prompt = f"""
You are a helpful customer service assistant.
A customer just said: \"{text}\"
The detected sentiment is {sentiment} and the intent is {intent}.

Based on this, give a real-time suggestion to the call center agent on how to respond.
Respond in one short actionable sentence.
"""
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=60,
            temperature=0.7
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"âš ï¸ GPT error: {e}"

# GPT summary helper
def summarize_call(text):
    prompt = f"Summarize this customer support call in 3â€“5 bullet points:\n{text}"
    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=200,
            temperature=0.5,
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"âš ï¸ Failed to summarize: {e}"

# Callback for audio stream
def callback(indata, frames, time, status):
    if status:
        print(status, flush=True)
    q.put(indata.copy())

# Main
print("\nAvailable input devices:")
for idx, dev in enumerate(sd.query_devices()):
    if dev["max_input_channels"] > 0:
        print(f"[{idx}] {dev['name']}")
mic_index = int(input("ğŸ‘‰ Enter mic index to use: "))
print(f"ğŸ™ï¸ Listening on {'cuda' if torch.cuda.is_available() else 'cpu'}...")

transcript_log = []

with sd.InputStream(callback=callback, channels=1, samplerate=RATE, blocksize=FRAME_SIZE, device=mic_index):
    triggered = False
    frames = []
    try:
        while True:
            frame = q.get()
            pcm_data = (frame * 32768).astype(np.int16).tobytes()
            is_speech = vad.is_speech(pcm_data, RATE)

            if is_speech:
                if not triggered:
                    print("ğŸ™ï¸ Voice detected, recording...")
                    triggered = True
                    frames = []
                frames.append(pcm_data)
            else:
                if triggered:
                    triggered = False
                    chunk = b''.join(frames)
                    duration = len(chunk) / 2 / RATE
                    float_data = np.frombuffer(chunk, dtype=np.int16).astype(np.float32) / 32768.0
                    rms = np.sqrt(np.mean(float_data**2))
                    db = 20 * np.log10(rms + 1e-8)
                    print(f"ğŸ›‘ Voice stopped, chunk duration: {duration:.2f}s, level: {db:.1f} dB")

                    if duration < MIN_CHUNK_DURATION:
                        print("âš ï¸ Chunk too short, skipping")
                        continue

                    # save to wav
                    fname = "debug_chunk.wav"
                    with wave.open(fname, 'wb') as wf:
                        wf.setnchannels(1)
                        wf.setsampwidth(2)
                        wf.setframerate(RATE)
                        wf.writeframes(chunk)
                    print(f"âœ… Saved {fname}")

                    # transcribe
                    try:
                        result = asr(fname)
                        text = result.get("text", "")
                        print(f"âœ… Final text: {text}")
                        transcript_log.append(text)

                        s = sentiment(text)[0]
                        print(f"Sentiment: {s}")

                        if intent_model:
                            inputs = tokenizer(text, return_tensors="pt").to(
                                "cuda" if torch.cuda.is_available() else "cpu"
                            )
                            with torch.no_grad():
                                logits = intent_model(**inputs).logits
                                probs = torch.softmax(logits, dim=1)
                                pred = torch.argmax(probs, dim=1).item()
                                label = label_encoder.inverse_transform([pred])[0]
                                print(f"Intent: {label}")
                        else:
                            label = "unknown"

                        suggestion = get_suggestion_from_gpt(label, s['label'], text)
                        print(f"ğŸ’¡ Suggestion: {suggestion}")

                    except Exception as e:
                        print(f"âš ï¸ Transcription failed: {e}")
    except KeyboardInterrupt:
        print("\nğŸ›‘ Call ended, generating summary...")
        full_conversation = " ".join(transcript_log)
        with open("full_transcript.txt", "w", encoding="utf-8") as f:
            f.write(full_conversation)
        summary = summarize_call(full_conversation)
        print("\nğŸ“‹ Summary:")
        print(summary)
        with open("call_summary.txt", "w", encoding="utf-8") as f:
            f.write(summary)
        sys.exit(0)
